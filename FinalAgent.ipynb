{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Running PPO on: MSFT\n",
      "Episode 50/500 â€” Reward 48.43\n",
      "Episode 100/500 â€” Reward 271.03\n",
      "Episode 150/500 â€” Reward 380.53\n",
      "Episode 200/500 â€” Reward 406.31\n",
      "Episode 250/500 â€” Reward 415.74\n",
      "Episode 300/500 â€” Reward 473.73\n",
      "Episode 350/500 â€” Reward 512.18\n",
      "Episode 400/500 â€” Reward 432.30\n",
      "Episode 450/500 â€” Reward 598.44\n",
      "Episode 500/500 â€” Reward 526.99\n",
      "ðŸ“ˆ MSFT â€” ROI: 5.82%, Sharpe: 3.28, Max Drawdown: 0.32%\n",
      "\n",
      "ðŸš€ Running PPO on: VZ\n",
      "Episode 50/500 â€” Reward 16.11\n",
      "Episode 100/500 â€” Reward 39.43\n",
      "Episode 150/500 â€” Reward 39.91\n",
      "Episode 200/500 â€” Reward 46.38\n",
      "Episode 250/500 â€” Reward 54.91\n",
      "Episode 300/500 â€” Reward 50.99\n",
      "Episode 350/500 â€” Reward 53.43\n",
      "Episode 400/500 â€” Reward 66.49\n",
      "Episode 450/500 â€” Reward 63.83\n",
      "Episode 500/500 â€” Reward 68.17\n",
      "ðŸ“ˆ VZ â€” ROI: 0.70%, Sharpe: 4.00, Max Drawdown: 0.04%\n",
      "\n",
      "ðŸš€ Running PPO on: KO\n",
      "Episode 50/500 â€” Reward 9.60\n",
      "Episode 100/500 â€” Reward 31.30\n",
      "Episode 150/500 â€” Reward 35.00\n",
      "Episode 200/500 â€” Reward 38.67\n",
      "Episode 250/500 â€” Reward 41.46\n",
      "Episode 300/500 â€” Reward 55.00\n",
      "Episode 350/500 â€” Reward 48.79\n",
      "Episode 400/500 â€” Reward 55.67\n",
      "Episode 450/500 â€” Reward 61.99\n",
      "Episode 500/500 â€” Reward 62.32\n",
      "ðŸ“ˆ KO â€” ROI: 0.58%, Sharpe: 3.08, Max Drawdown: 0.05%\n",
      "\n",
      "ðŸš€ Running PPO on: HD\n",
      "Episode 50/500 â€” Reward 157.03\n",
      "Episode 100/500 â€” Reward 237.37\n",
      "Episode 150/500 â€” Reward 249.06\n",
      "Episode 200/500 â€” Reward 356.71\n",
      "Episode 250/500 â€” Reward 328.22\n",
      "Episode 300/500 â€” Reward 293.91\n",
      "Episode 350/500 â€” Reward 412.19\n",
      "Episode 400/500 â€” Reward 429.22\n",
      "Episode 450/500 â€” Reward 405.85\n",
      "Episode 500/500 â€” Reward 443.59\n",
      "ðŸ“ˆ HD â€” ROI: 4.14%, Sharpe: 2.61, Max Drawdown: 0.40%\n",
      "\n",
      "ðŸš€ Running PPO on: V\n",
      "Episode 50/500 â€” Reward 127.85\n",
      "Episode 100/500 â€” Reward 147.37\n",
      "Episode 150/500 â€” Reward 244.54\n",
      "Episode 200/500 â€” Reward 204.64\n",
      "Episode 250/500 â€” Reward 263.14\n",
      "Episode 300/500 â€” Reward 290.49\n",
      "Episode 350/500 â€” Reward 265.56\n",
      "Episode 400/500 â€” Reward 302.13\n",
      "Episode 450/500 â€” Reward 303.85\n",
      "Episode 500/500 â€” Reward 293.93\n",
      "ðŸ“ˆ V â€” ROI: 3.09%, Sharpe: 2.68, Max Drawdown: 0.21%\n",
      "\n",
      "ðŸš€ Running PPO on: MMM\n",
      "Episode 50/500 â€” Reward 30.44\n",
      "Episode 100/500 â€” Reward 82.15\n",
      "Episode 150/500 â€” Reward 70.57\n",
      "Episode 200/500 â€” Reward 114.44\n",
      "Episode 250/500 â€” Reward 104.03\n",
      "Episode 300/500 â€” Reward 136.12\n",
      "Episode 350/500 â€” Reward 149.69\n",
      "Episode 400/500 â€” Reward 143.41\n",
      "Episode 450/500 â€” Reward 172.15\n",
      "Episode 500/500 â€” Reward 144.45\n",
      "ðŸ“ˆ MMM â€” ROI: 1.65%, Sharpe: 2.38, Max Drawdown: 0.14%\n",
      "\n",
      "ðŸš€ Running PPO on: PFE\n",
      "Episode 50/500 â€” Reward 5.28\n",
      "Episode 100/500 â€” Reward 11.42\n",
      "Episode 150/500 â€” Reward 10.44\n",
      "Episode 200/500 â€” Reward 23.10\n",
      "Episode 250/500 â€” Reward 18.87\n",
      "Episode 300/500 â€” Reward 30.15\n",
      "Episode 350/500 â€” Reward 29.27\n",
      "Episode 400/500 â€” Reward 25.77\n",
      "Episode 450/500 â€” Reward 31.07\n",
      "Episode 500/500 â€” Reward 35.04\n",
      "ðŸ“ˆ PFE â€” ROI: 0.35%, Sharpe: 2.98, Max Drawdown: 0.01%\n",
      "\n",
      "ðŸš€ Running PPO on: NKE\n",
      "Episode 50/500 â€” Reward 36.87\n",
      "Episode 100/500 â€” Reward 58.74\n",
      "Episode 150/500 â€” Reward 80.15\n",
      "Episode 200/500 â€” Reward 88.01\n",
      "Episode 250/500 â€” Reward 124.47\n",
      "Episode 300/500 â€” Reward 133.13\n",
      "Episode 350/500 â€” Reward 93.93\n",
      "Episode 400/500 â€” Reward 130.38\n",
      "Episode 450/500 â€” Reward 139.68\n",
      "Episode 500/500 â€” Reward 180.23\n",
      "ðŸ“ˆ NKE â€” ROI: 1.58%, Sharpe: 2.48, Max Drawdown: 0.10%\n",
      "\n",
      "ðŸš€ Running PPO on: CAT\n",
      "Episode 50/500 â€” Reward 208.10\n",
      "Episode 100/500 â€” Reward 174.17\n",
      "Episode 150/500 â€” Reward 332.93\n",
      "Episode 200/500 â€” Reward 314.39\n",
      "Episode 250/500 â€” Reward 305.36\n",
      "Episode 300/500 â€” Reward 358.21\n",
      "Episode 350/500 â€” Reward 465.39\n",
      "Episode 400/500 â€” Reward 456.63\n",
      "Episode 450/500 â€” Reward 464.68\n",
      "Episode 500/500 â€” Reward 527.06\n",
      "ðŸ“ˆ CAT â€” ROI: 5.47%, Sharpe: 3.09, Max Drawdown: 0.30%\n",
      "\n",
      "ðŸš€ Running PPO on: GS\n",
      "Episode 50/500 â€” Reward 337.20\n",
      "Episode 100/500 â€” Reward 466.72\n",
      "Episode 150/500 â€” Reward 458.94\n",
      "Episode 200/500 â€” Reward 345.81\n",
      "Episode 250/500 â€” Reward 572.90\n",
      "Episode 300/500 â€” Reward 565.23\n",
      "Episode 350/500 â€” Reward 635.36\n",
      "Episode 400/500 â€” Reward 567.50\n",
      "Episode 450/500 â€” Reward 584.04\n",
      "Episode 500/500 â€” Reward 604.83\n",
      "ðŸ“ˆ GS â€” ROI: 6.17%, Sharpe: 2.70, Max Drawdown: 0.34%\n",
      "\n",
      "ðŸš€ Running PPO on: JNJ\n",
      "Episode 50/500 â€” Reward -20.04\n",
      "Episode 100/500 â€” Reward 54.66\n",
      "Episode 150/500 â€” Reward 70.06\n",
      "Episode 200/500 â€” Reward 82.63\n",
      "Episode 250/500 â€” Reward 82.16\n",
      "Episode 300/500 â€” Reward 126.74\n",
      "Episode 350/500 â€” Reward 115.23\n",
      "Episode 400/500 â€” Reward 137.54\n",
      "Episode 450/500 â€” Reward 129.34\n",
      "Episode 500/500 â€” Reward 142.12\n",
      "ðŸ“ˆ JNJ â€” ROI: 1.70%, Sharpe: 2.93, Max Drawdown: 0.09%\n",
      "\n",
      "ðŸš€ Running PPO on: DD\n",
      "Episode 50/500 â€” Reward 26.38\n",
      "Episode 100/500 â€” Reward 31.12\n",
      "Episode 150/500 â€” Reward 58.70\n",
      "Episode 200/500 â€” Reward 70.03\n",
      "Episode 250/500 â€” Reward 83.12\n",
      "Episode 300/500 â€” Reward 79.28\n",
      "Episode 350/500 â€” Reward 93.57\n",
      "Episode 400/500 â€” Reward 103.89\n",
      "Episode 450/500 â€” Reward 107.98\n",
      "Episode 500/500 â€” Reward 109.04\n",
      "ðŸ“ˆ DD â€” ROI: 1.12%, Sharpe: 2.89, Max Drawdown: 0.11%\n",
      "\n",
      "ðŸš€ Running PPO on: TRV\n",
      "Episode 50/500 â€” Reward 66.39\n",
      "Episode 100/500 â€” Reward 161.71\n",
      "Episode 150/500 â€” Reward 100.33\n",
      "Episode 200/500 â€” Reward 108.12\n",
      "Episode 250/500 â€” Reward 125.34\n",
      "Episode 300/500 â€” Reward 166.25\n",
      "Episode 350/500 â€” Reward 145.37\n",
      "Episode 400/500 â€” Reward 172.36\n",
      "Episode 450/500 â€” Reward 200.74\n",
      "Episode 500/500 â€” Reward 189.34\n",
      "ðŸ“ˆ TRV â€” ROI: 1.96%, Sharpe: 2.37, Max Drawdown: 0.18%\n",
      "\n",
      "ðŸš€ Running PPO on: JPM\n",
      "Episode 50/500 â€” Reward 73.92\n",
      "Episode 100/500 â€” Reward 107.10\n",
      "Episode 150/500 â€” Reward 142.05\n",
      "Episode 200/500 â€” Reward 159.20\n",
      "Episode 250/500 â€” Reward 150.08\n",
      "Episode 300/500 â€” Reward 169.20\n",
      "Episode 350/500 â€” Reward 211.65\n",
      "Episode 400/500 â€” Reward 191.75\n",
      "Episode 450/500 â€” Reward 244.31\n",
      "Episode 500/500 â€” Reward 230.01\n",
      "ðŸ“ˆ JPM â€” ROI: 2.14%, Sharpe: 2.26, Max Drawdown: 0.22%\n",
      "\n",
      "ðŸš€ Running PPO on: WMT\n",
      "Episode 50/500 â€” Reward 73.55\n",
      "Episode 100/500 â€” Reward 98.33\n",
      "Episode 150/500 â€” Reward 86.33\n",
      "Episode 200/500 â€” Reward 77.35\n",
      "Episode 250/500 â€” Reward 120.15\n",
      "Episode 300/500 â€” Reward 104.59\n",
      "Episode 350/500 â€” Reward 105.84\n",
      "Episode 400/500 â€” Reward 105.37\n",
      "Episode 450/500 â€” Reward 108.07\n",
      "Episode 500/500 â€” Reward 126.18\n",
      "ðŸ“ˆ WMT â€” ROI: 1.04%, Sharpe: 1.50, Max Drawdown: 0.19%\n",
      "\n",
      "ðŸš€ Running PPO on: DIS\n",
      "Episode 50/500 â€” Reward 56.71\n",
      "Episode 100/500 â€” Reward 84.56\n",
      "Episode 150/500 â€” Reward 89.17\n",
      "Episode 200/500 â€” Reward 123.26\n",
      "Episode 250/500 â€” Reward 134.81\n",
      "Episode 300/500 â€” Reward 127.65\n",
      "Episode 350/500 â€” Reward 125.33\n",
      "Episode 400/500 â€” Reward 130.26\n",
      "Episode 450/500 â€” Reward 159.73\n",
      "Episode 500/500 â€” Reward 150.09\n",
      "ðŸ“ˆ DIS â€” ROI: 1.39%, Sharpe: 2.55, Max Drawdown: 0.17%\n",
      "\n",
      "ðŸš€ Running PPO on: WBA\n",
      "Episode 50/500 â€” Reward 2.85\n",
      "Episode 100/500 â€” Reward 4.96\n",
      "Episode 150/500 â€” Reward 18.14\n",
      "Episode 200/500 â€” Reward 14.11\n",
      "Episode 250/500 â€” Reward 21.06\n",
      "Episode 300/500 â€” Reward 21.46\n",
      "Episode 350/500 â€” Reward 22.15\n",
      "Episode 400/500 â€” Reward 27.03\n",
      "Episode 450/500 â€” Reward 25.76\n",
      "Episode 500/500 â€” Reward 28.94\n",
      "ðŸ“ˆ WBA â€” ROI: 0.31%, Sharpe: 2.11, Max Drawdown: 0.03%\n",
      "\n",
      "ðŸš€ Running PPO on: UNH\n",
      "Episode 50/500 â€” Reward 247.56\n",
      "Episode 100/500 â€” Reward 317.28\n",
      "Episode 150/500 â€” Reward 501.93\n",
      "Episode 200/500 â€” Reward 567.16\n",
      "Episode 250/500 â€” Reward 649.45\n",
      "Episode 300/500 â€” Reward 682.77\n",
      "Episode 350/500 â€” Reward 725.66\n",
      "Episode 400/500 â€” Reward 810.03\n",
      "Episode 450/500 â€” Reward 862.38\n",
      "Episode 500/500 â€” Reward 726.22\n",
      "ðŸ“ˆ UNH â€” ROI: 8.85%, Sharpe: 3.73, Max Drawdown: 0.34%\n",
      "\n",
      "ðŸš€ Running PPO on: MCD\n",
      "Episode 50/500 â€” Reward 93.90\n",
      "Episode 100/500 â€” Reward 124.09\n",
      "Episode 150/500 â€” Reward 125.89\n",
      "Episode 200/500 â€” Reward 148.32\n",
      "Episode 250/500 â€” Reward 191.28\n",
      "Episode 300/500 â€” Reward 201.30\n",
      "Episode 350/500 â€” Reward 210.55\n",
      "Episode 400/500 â€” Reward 196.97\n",
      "Episode 450/500 â€” Reward 240.84\n",
      "Episode 500/500 â€” Reward 250.69\n",
      "ðŸ“ˆ MCD â€” ROI: 2.19%, Sharpe: 2.36, Max Drawdown: 0.31%\n",
      "\n",
      "ðŸš€ Running PPO on: AXP\n",
      "Episode 50/500 â€” Reward 142.38\n",
      "Episode 100/500 â€” Reward 179.06\n",
      "Episode 150/500 â€” Reward 167.41\n",
      "Episode 200/500 â€” Reward 279.81\n",
      "Episode 250/500 â€” Reward 258.31\n",
      "Episode 300/500 â€” Reward 319.79\n",
      "Episode 350/500 â€” Reward 337.01\n",
      "Episode 400/500 â€” Reward 327.56\n",
      "Episode 450/500 â€” Reward 337.57\n",
      "Episode 500/500 â€” Reward 330.13\n",
      "ðŸ“ˆ AXP â€” ROI: 3.46%, Sharpe: 2.93, Max Drawdown: 0.17%\n",
      "\n",
      "ðŸš€ Running PPO on: BA\n",
      "Episode 50/500 â€” Reward 142.62\n",
      "Episode 100/500 â€” Reward 202.64\n",
      "Episode 150/500 â€” Reward 224.27\n",
      "Episode 200/500 â€” Reward 198.29\n",
      "Episode 250/500 â€” Reward 229.56\n",
      "Episode 300/500 â€” Reward 207.07\n",
      "Episode 350/500 â€” Reward 212.46\n",
      "Episode 400/500 â€” Reward 217.31\n",
      "Episode 450/500 â€” Reward 214.71\n",
      "Episode 500/500 â€” Reward 212.36\n",
      "ðŸ“ˆ BA â€” ROI: 2.37%, Sharpe: 2.50, Max Drawdown: 0.25%\n",
      "\n",
      "ðŸš€ Running PPO on: XOM\n",
      "Episode 50/500 â€” Reward 84.77\n",
      "Episode 100/500 â€” Reward 147.74\n",
      "Episode 150/500 â€” Reward 146.82\n",
      "Episode 200/500 â€” Reward 154.44\n",
      "Episode 250/500 â€” Reward 186.01\n",
      "Episode 300/500 â€” Reward 186.74\n",
      "Episode 350/500 â€” Reward 186.19\n",
      "Episode 400/500 â€” Reward 205.76\n",
      "Episode 450/500 â€” Reward 217.54\n",
      "Episode 500/500 â€” Reward 229.17\n",
      "ðŸ“ˆ XOM â€” ROI: 2.48%, Sharpe: 4.16, Max Drawdown: 0.08%\n",
      "\n",
      "ðŸš€ Running PPO on: INTC\n",
      "Episode 50/500 â€” Reward 16.92\n",
      "Episode 100/500 â€” Reward 27.65\n",
      "Episode 150/500 â€” Reward 30.24\n",
      "Episode 200/500 â€” Reward 53.49\n",
      "Episode 250/500 â€” Reward 47.41\n",
      "Episode 300/500 â€” Reward 59.62\n",
      "Episode 350/500 â€” Reward 49.43\n",
      "Episode 400/500 â€” Reward 59.08\n",
      "Episode 450/500 â€” Reward 65.75\n",
      "Episode 500/500 â€” Reward 60.00\n",
      "ðŸ“ˆ INTC â€” ROI: 0.71%, Sharpe: 3.60, Max Drawdown: 0.02%\n",
      "\n",
      "ðŸš€ Running PPO on: RTX\n",
      "Episode 50/500 â€” Reward 42.37\n",
      "Episode 100/500 â€” Reward 47.79\n",
      "Episode 150/500 â€” Reward 87.14\n",
      "Episode 200/500 â€” Reward 79.41\n",
      "Episode 250/500 â€” Reward 104.78\n",
      "Episode 300/500 â€” Reward 124.00\n",
      "Episode 350/500 â€” Reward 120.76\n",
      "Episode 400/500 â€” Reward 116.44\n",
      "Episode 450/500 â€” Reward 139.77\n",
      "Episode 500/500 â€” Reward 134.61\n",
      "ðŸ“ˆ RTX â€” ROI: 1.37%, Sharpe: 3.13, Max Drawdown: 0.12%\n",
      "\n",
      "ðŸš€ Running PPO on: CVX\n",
      "Episode 50/500 â€” Reward 142.15\n",
      "Episode 100/500 â€” Reward 195.34\n",
      "Episode 150/500 â€” Reward 169.21\n",
      "Episode 200/500 â€” Reward 244.40\n",
      "Episode 250/500 â€” Reward 243.79\n",
      "Episode 300/500 â€” Reward 236.25\n",
      "Episode 350/500 â€” Reward 265.09\n",
      "Episode 400/500 â€” Reward 286.50\n",
      "Episode 450/500 â€” Reward 264.50\n",
      "Episode 500/500 â€” Reward 277.78\n",
      "ðŸ“ˆ CVX â€” ROI: 2.80%, Sharpe: 3.52, Max Drawdown: 0.17%\n",
      "\n",
      "ðŸš€ Running PPO on: PG\n",
      "Episode 50/500 â€” Reward 94.39\n",
      "Episode 100/500 â€” Reward 118.25\n",
      "Episode 150/500 â€” Reward 124.97\n",
      "Episode 200/500 â€” Reward 114.41\n",
      "Episode 250/500 â€” Reward 125.91\n",
      "Episode 300/500 â€” Reward 156.63\n",
      "Episode 350/500 â€” Reward 174.34\n",
      "Episode 400/500 â€” Reward 167.55\n",
      "Episode 450/500 â€” Reward 193.14\n",
      "Episode 500/500 â€” Reward 175.69\n",
      "ðŸ“ˆ PG â€” ROI: 1.85%, Sharpe: 3.19, Max Drawdown: 0.09%\n",
      "\n",
      "ðŸš€ Running PPO on: IBM\n",
      "Episode 50/500 â€” Reward 82.31\n",
      "Episode 100/500 â€” Reward 156.11\n",
      "Episode 150/500 â€” Reward 186.44\n",
      "Episode 200/500 â€” Reward 175.71\n",
      "Episode 250/500 â€” Reward 174.98\n",
      "Episode 300/500 â€” Reward 182.17\n",
      "Episode 350/500 â€” Reward 189.11\n",
      "Episode 400/500 â€” Reward 196.95\n",
      "Episode 450/500 â€” Reward 224.13\n",
      "Episode 500/500 â€” Reward 195.12\n",
      "ðŸ“ˆ IBM â€” ROI: 1.93%, Sharpe: 2.47, Max Drawdown: 0.13%\n",
      "\n",
      "ðŸš€ Running PPO on: AAPL\n",
      "Episode 50/500 â€” Reward 102.29\n",
      "Episode 100/500 â€” Reward 136.50\n",
      "Episode 150/500 â€” Reward 173.51\n",
      "Episode 200/500 â€” Reward 225.38\n",
      "Episode 250/500 â€” Reward 225.25\n",
      "Episode 300/500 â€” Reward 239.82\n",
      "Episode 350/500 â€” Reward 289.51\n",
      "Episode 400/500 â€” Reward 293.03\n",
      "Episode 450/500 â€” Reward 318.00\n",
      "Episode 500/500 â€” Reward 318.04\n",
      "ðŸ“ˆ AAPL â€” ROI: 3.15%, Sharpe: 2.87, Max Drawdown: 0.25%\n",
      "\n",
      "ðŸš€ Running PPO on: MRK\n",
      "Episode 50/500 â€” Reward 55.55\n",
      "Episode 100/500 â€” Reward 88.60\n",
      "Episode 150/500 â€” Reward 91.37\n",
      "Episode 200/500 â€” Reward 107.81\n",
      "Episode 250/500 â€” Reward 111.73\n",
      "Episode 300/500 â€” Reward 130.86\n",
      "Episode 350/500 â€” Reward 137.06\n",
      "Episode 400/500 â€” Reward 139.84\n",
      "Episode 450/500 â€” Reward 125.43\n",
      "Episode 500/500 â€” Reward 140.22\n",
      "ðŸ“ˆ MRK â€” ROI: 1.48%, Sharpe: 2.84, Max Drawdown: 0.14%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from gym import spaces\n",
    "import csv\n",
    "\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "auto_seed = 42\n",
    "np.random.seed(auto_seed)\n",
    "torch.manual_seed(auto_seed)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 1. Trading Environment Including Standardization, Sentiment Score, Forecast\n",
    "# =============================================================================\n",
    "class TradingEnv(gym.Env):\n",
    "   \"\"\"\n",
    "   A custom trading environment that includes:\n",
    "     - sentiment score,\n",
    "     - forecasted price,\n",
    "     - standard ATR stop-loss,\n",
    "     - z-score standardization of features.\n",
    "\n",
    "\n",
    "   Expected CSV columns:\n",
    "     - Date, High, Low, Price (close), MA5, MA20, RSI, MACD,\n",
    "       sentiment_score, Forecasted_Price\n",
    "\n",
    "\n",
    "   State vector (length = n_features + 1):\n",
    "     [z(Price), z(MA5), z(MA20), z(RSI), z(MACD),\n",
    "      z(sentiment_score), z(Forecasted_Price), Position]\n",
    "   where Position is 0 or 1.\n",
    "\n",
    "\n",
    "   Actions:\n",
    "     0 = Hold, 1 = Buy, 2 = Sell\n",
    "   \"\"\"\n",
    "   metadata = {'render.modes': ['human']}\n",
    "\n",
    "\n",
    "   def __init__(self,\n",
    "                data_path=\"/Users/User/Desktop/DIA/FinancialTradingBot/Merged_Data_For_RL/\"\n",
    "                          \"AAPL_stock_data_2022-01-01_to_2024-12-31.csv\",\n",
    "                initial_balance=10000,\n",
    "                stop_loss_multiplier=2,\n",
    "                window=5):\n",
    "       super(TradingEnv, self).__init__()\n",
    "       # 1) Load data and parse dates\n",
    "       self.data = pd.read_csv(data_path)\n",
    "       self.data.rename(columns={\n",
    "            \"Close\": \"Price\",\n",
    "            \"Sentiment_Score\": \"sentiment_score\",\n",
    "            \"Predicted_Next_Close\": \"Forecasted_Price\"\n",
    "        }, inplace=True)\n",
    "\n",
    "       self.data[\"Date\"] = pd.to_datetime(self.data[\"Date\"])\n",
    "       self.num_data = len(self.data)\n",
    "\n",
    "\n",
    "       # 2) Portfolio params\n",
    "       self.initial_balance      = initial_balance\n",
    "       self.stop_loss_multiplier = stop_loss_multiplier\n",
    "       self.window               = window\n",
    "\n",
    "\n",
    "       # 3) Compute z-score stats for features\n",
    "       feature_cols = [\n",
    "           \"Price\", \"MA5\", \"MA20\",\n",
    "           \"RSI\", \"MACD\",\n",
    "           \"sentiment_score\", \"Forecasted_Price\"\n",
    "       ]\n",
    "       self.feature_cols  = feature_cols\n",
    "       self.feature_mean  = self.data[feature_cols].mean()\n",
    "       self.feature_std   = self.data[feature_cols].std()\n",
    "\n",
    "\n",
    "       # 4) Action & Observation spaces\n",
    "       self.action_space = spaces.Discrete(3)  # 0=Hold,1=Buy,2=Sell\n",
    "       state_dim = len(feature_cols) + 1       # +1 for Position\n",
    "       self.observation_space = spaces.Box(\n",
    "           low=-np.inf,\n",
    "           high=np.inf,\n",
    "           shape=(state_dim,),\n",
    "           dtype=np.float32\n",
    "       )\n",
    "\n",
    "\n",
    "       # 5) Initialize state\n",
    "       _ = self.reset()\n",
    "\n",
    "\n",
    "   def reset(self):\n",
    "       \"\"\"\n",
    "       Resets portfolio and time pointer; returns first observation.\n",
    "       \"\"\"\n",
    "       self.balance        = self.initial_balance\n",
    "       self.shares         = 0\n",
    "       self.position       = 0\n",
    "       self.entry_price    = 0.0\n",
    "       self.current_step   = 0\n",
    "       self.done           = False\n",
    "       self.equity_history = [self.initial_balance]\n",
    "       return self._get_state()\n",
    "\n",
    "\n",
    "   def _get_state(self):\n",
    "       row = self.data.iloc[self.current_step]\n",
    "       vals = (row[self.feature_cols] - self.feature_mean) / self.feature_std\n",
    "       state = np.concatenate([vals.values, [self.position]])\n",
    "       return state.astype(np.float32)\n",
    "\n",
    "\n",
    "   def step(self, action):\n",
    "        if self.done:\n",
    "            raise RuntimeError(\"Episode has ended. Please reset().\")\n",
    "\n",
    "        executed_action = \"Hold\"  # Default\n",
    "\n",
    "        # Force Hold if trying to Buy when already holding\n",
    "        if self.shares == 1 and action == 1:\n",
    "            action = 0\n",
    "\n",
    "        prev_close  = self.data.loc[self.current_step, \"Price\"]\n",
    "        prev_equity = self.balance + self.shares * prev_close\n",
    "\n",
    "        # advance timestep\n",
    "        self.current_step += 1\n",
    "        if self.current_step >= self.num_data - 1:\n",
    "            self.done = True\n",
    "\n",
    "        current_close = self.data.loc[self.current_step, \"Price\"]\n",
    "\n",
    "        # Execute action\n",
    "        if action == 1 and self.shares == 0 and self.balance >= current_close:\n",
    "            self.shares      = 1\n",
    "            self.position    = 1\n",
    "            self.entry_price = current_close\n",
    "            self.balance    -= current_close\n",
    "            executed_action = \"Buy\"\n",
    "        elif action == 2 and self.shares > 0:\n",
    "            self.balance    += current_close * self.shares\n",
    "            self.shares      = 0\n",
    "            self.position    = 0\n",
    "            self.entry_price = 0.0\n",
    "            executed_action = \"Sell\"\n",
    "\n",
    "        # ATR stop-loss\n",
    "        if self.shares > 0:\n",
    "            trs = []\n",
    "            start = max(1, self.current_step - self.window + 1)\n",
    "            for i in range(start, self.current_step + 1):\n",
    "                hi = self.data.loc[i,   \"High\"]\n",
    "                lo = self.data.loc[i,   \"Low\"]\n",
    "                prev_c = self.data.loc[i-1, \"Price\"]\n",
    "                tr = max(hi - lo, abs(hi - prev_c), abs(lo - prev_c))\n",
    "                trs.append(tr)\n",
    "            atr = np.mean(trs) if trs else 0.0\n",
    "            if current_close < self.entry_price - self.stop_loss_multiplier * atr:\n",
    "                self.balance    += current_close * self.shares\n",
    "                self.shares      = 0\n",
    "                self.position    = 0\n",
    "                self.entry_price = 0.0\n",
    "                executed_action = \"Sell\"\n",
    "\n",
    "        # Reward\n",
    "        equity = self.balance + self.shares * current_close\n",
    "        reward = equity - prev_equity\n",
    "        self.equity_history.append(equity)\n",
    "\n",
    "        return self._get_state(), reward, self.done, {\"executed_action\": executed_action}\n",
    "\n",
    "\n",
    "\n",
    "   def render(self, action=None, mode='human'):\n",
    "       \"\"\"\n",
    "       Prints the current trading date, action taken, price and portfolio state.\n",
    "       \"\"\"\n",
    "       row      = self.data.iloc[self.current_step]\n",
    "       date_str = row[\"Date\"].strftime(\"%Y-%m-%d\")\n",
    "       price    = row[\"Price\"]\n",
    "       equity   = self.balance + self.shares * price\n",
    "       action_str = {0: 'Hold', 1: 'Buy', 2: 'Sell'}.get(action, 'N/A')\n",
    "\n",
    "\n",
    "       print(f\"Date: {date_str},  \"\n",
    "             f\"Action: {action_str},  \"\n",
    "             f\"Price: {price:.2f},  \"\n",
    "             f\"Balance: {self.balance:.2f},  \"\n",
    "             f\"Shares: {self.shares},  \"\n",
    "             f\"Equity: {equity:.2f},  \"\n",
    "             f\"Position: {self.position}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2. PPO Actor-Critic Network\n",
    "# =============================================================================\n",
    "class ActorCritic(nn.Module):\n",
    "   def __init__(self, input_dim, action_dim, hidden_dim=64):\n",
    "       super().__init__()\n",
    "       self.shared = nn.Sequential(\n",
    "           nn.Linear(input_dim, hidden_dim),\n",
    "           nn.ReLU()\n",
    "       )\n",
    "       self.actor = nn.Sequential(\n",
    "           nn.Linear(hidden_dim, hidden_dim),\n",
    "           nn.ReLU(),\n",
    "           nn.Linear(hidden_dim, action_dim),\n",
    "           nn.Softmax(dim=-1)\n",
    "       )\n",
    "       self.critic = nn.Sequential(\n",
    "           nn.Linear(hidden_dim, hidden_dim),\n",
    "           nn.ReLU(),\n",
    "           nn.Linear(hidden_dim, 1)\n",
    "       )\n",
    "\n",
    "\n",
    "   def forward(self, x):\n",
    "       h = self.shared(x)\n",
    "       return self.actor(h), self.critic(h)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3. PPO Agent\n",
    "# =============================================================================\n",
    "class PPOAgent:\n",
    "   def __init__(self, input_dim, action_dim,\n",
    "                hidden_dim=64, lr=3e-4,\n",
    "                gamma=0.99, lam=0.95,\n",
    "                clip_epsilon=0.2,\n",
    "                update_epochs=10,\n",
    "                batch_size=64):\n",
    "       self.gamma = gamma\n",
    "       self.lam = lam\n",
    "       self.clip_epsilon = clip_epsilon\n",
    "       self.update_epochs = update_epochs\n",
    "       self.batch_size = batch_size\n",
    "\n",
    "\n",
    "       self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "       self.model = ActorCritic(input_dim, action_dim, hidden_dim).to(self.device)\n",
    "       self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "   def select_action(self, state):\n",
    "       s = torch.FloatTensor(state).to(self.device)\n",
    "       probs, val = self.model(s)\n",
    "       dist = torch.distributions.Categorical(probs)\n",
    "       a = dist.sample()\n",
    "       return a.item(), dist.log_prob(a).item(), val.item()\n",
    "\n",
    "\n",
    "   def compute_gae(self, rewards, values, dones):\n",
    "       advantages = []\n",
    "       gae = 0\n",
    "       values = values + [0]\n",
    "       for t in reversed(range(len(rewards))):\n",
    "           delta = rewards[t] + self.gamma * (1 - dones[t]) * values[t+1] - values[t]\n",
    "           gae = delta + self.gamma * self.lam * (1 - dones[t]) * gae\n",
    "           advantages.insert(0, gae)\n",
    "       returns = [adv + v for adv, v in zip(advantages, values[:-1])]\n",
    "       return advantages, returns\n",
    "\n",
    "\n",
    "   def update(self, traj):\n",
    "       states      = torch.FloatTensor(traj['states']).to(self.device)\n",
    "       actions     = torch.LongTensor(traj['actions']).to(self.device)\n",
    "       old_logprobs= torch.FloatTensor(traj['log_probs']).to(self.device)\n",
    "       rewards     = traj['rewards']\n",
    "       dones       = traj['dones']\n",
    "       values      = traj['values']\n",
    "\n",
    "\n",
    "       advs, rets = self.compute_gae(rewards, values, dones)\n",
    "       advs = torch.FloatTensor(advs).to(self.device)\n",
    "       rets = torch.FloatTensor(rets).to(self.device)\n",
    "       advs = (advs - advs.mean()) / (advs.std() + 1e-8)\n",
    "\n",
    "\n",
    "       dataset_size = states.size(0)\n",
    "       for _ in range(self.update_epochs):\n",
    "           perm = torch.randperm(dataset_size)\n",
    "           for i in range(0, dataset_size, self.batch_size):\n",
    "               idx = perm[i:i+self.batch_size]\n",
    "               bs, ba, blp, ba_adv, br = (\n",
    "                   states[idx], actions[idx], old_logprobs[idx], advs[idx], rets[idx]\n",
    "               )\n",
    "               probs, vals = self.model(bs)\n",
    "               dist = torch.distributions.Categorical(probs)\n",
    "               lp   = dist.log_prob(ba)\n",
    "               ent  = dist.entropy().mean()\n",
    "\n",
    "\n",
    "               ratio = torch.exp(lp - blp)\n",
    "               s1    = ratio * ba_adv\n",
    "               s2    = torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * ba_adv\n",
    "               actor_loss = -torch.min(s1, s2).mean()\n",
    "               critic_loss= nn.MSELoss()(vals.squeeze(-1), br)\n",
    "               loss       = actor_loss + 0.5 * critic_loss - 0.01 * ent\n",
    "\n",
    "\n",
    "               self.optimizer.zero_grad()\n",
    "               loss.backward()\n",
    "               self.optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 4. PPO Training Loop\n",
    "# =============================================================================\n",
    "def train_ppo(env, agent, num_episodes=500, rollout_length=None):\n",
    "   rewards_hist = []\n",
    "   rollout_length = rollout_length or env.num_data\n",
    "   for ep in range(num_episodes):\n",
    "       state = env.reset()\n",
    "       traj = {'states': [], 'actions': [], 'log_probs': [], 'rewards': [], 'dones': [], 'values': []}\n",
    "       total, done = 0, False\n",
    "       for _ in range(rollout_length):\n",
    "           a, lp, v = agent.select_action(state)\n",
    "           ns, r, done, _ = env.step(a)\n",
    "           traj['states'].append(state)\n",
    "           traj['actions'].append(a)\n",
    "           traj['log_probs'].append(lp)\n",
    "           traj['rewards'].append(r)\n",
    "           traj['dones'].append(float(done))\n",
    "           traj['values'].append(v)\n",
    "           state = ns\n",
    "           total += r\n",
    "           if done:\n",
    "               break\n",
    "       agent.update(traj)\n",
    "       rewards_hist.append(total)\n",
    "       if (ep+1) % 50 == 0:\n",
    "           print(f\"Episode {ep+1}/{num_episodes} â€” Reward {total:.2f}\")\n",
    "   return rewards_hist\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 5. Performance Metrics\n",
    "# =============================================================================\n",
    "def compute_performance_metrics(equity_history, risk_free_rate=0.0):\n",
    "   e = np.array(equity_history)\n",
    "   roi = (e[-1] - e[0]) / e[0]\n",
    "   dr = np.diff(e) / e[:-1]\n",
    "   mu, sigma = dr.mean(), dr.std()\n",
    "   sr = ((mu - risk_free_rate) / sigma * np.sqrt(252)) if sigma > 0 else 0.0\n",
    "   peak, maxdd = e[0], 0\n",
    "   for x in e:\n",
    "       peak = max(peak, x)\n",
    "       dd = (peak - x) / peak\n",
    "       maxdd = max(maxdd, dd)\n",
    "   return roi, sr, maxdd\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 6. Main: Train, Eval, and Plot with date printing\n",
    "# =============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    folder_path = \"/Users/wongyule/Documents/Designing Intelligent Agents/FinancialTradingBot/Merged_Data_For_RL\"  # CHANGE THIS to your directory\n",
    "    output_folder = \"logs\"  # log output folder\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if not filename.endswith(\".csv\"):\n",
    "            continue\n",
    "\n",
    "        symbol = filename.split(\"_\")[0]  # e.g., AAPL_stock_data_... â†’ AAPL\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "        print(f\"\\nðŸš€ Running PPO on: {symbol}\")\n",
    "\n",
    "        # Create environment and agent\n",
    "        env = TradingEnv(data_path=file_path)\n",
    "        state_dim = env.observation_space.shape[0]\n",
    "        action_dim = env.action_space.n\n",
    "        agent = PPOAgent(input_dim=state_dim, action_dim=action_dim)\n",
    "\n",
    "        # Train PPO\n",
    "        rewards = train_ppo(env, agent, num_episodes=500, rollout_length=env.num_data)\n",
    "\n",
    "        # Evaluate and collect logs\n",
    "        state, done = env.reset(), False\n",
    "        log_rows = []\n",
    "\n",
    "        while not done:\n",
    "            action, log_prob, value = agent.select_action(state)\n",
    "            obs_row = env.data.iloc[env.current_step]\n",
    "            price = obs_row[\"Price\"]\n",
    "            equity = env.balance + env.shares * price\n",
    "            feature_vals = (obs_row[env.feature_cols] - env.feature_mean) / env.feature_std\n",
    "\n",
    "            state, _, done, info = env.step(action)\n",
    "\n",
    "            log_rows.append({\n",
    "                \"Date\": obs_row[\"Date\"].strftime(\"%Y-%m-%d\"),\n",
    "                **{f\"z_{col}\": feature_vals[col] for col in env.feature_cols},\n",
    "                \"Action\": info[\"executed_action\"],\n",
    "                \"Price\": price,\n",
    "                \"Forecasted_Price\": obs_row.get(\"Forecasted_Price\", np.nan),\n",
    "                \"Balance\": env.balance,\n",
    "                \"Shares\": env.shares,\n",
    "                \"Equity\": equity,\n",
    "                \"Position\": env.position\n",
    "            })\n",
    "\n",
    "\n",
    "        # Save log\n",
    "        log_csv = os.path.join(output_folder, f\"ppo_trading_log_{symbol}.csv\")\n",
    "        with open(log_csv, mode='w', newline='') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=log_rows[0].keys())\n",
    "            writer.writeheader()\n",
    "            writer.writerows(log_rows)\n",
    "\n",
    "        # Print metrics\n",
    "        roi, sharpe, maxdd = compute_performance_metrics(env.equity_history)\n",
    "        print(f\"ðŸ“ˆ {symbol} â€” ROI: {roi*100:.2f}%, Sharpe: {sharpe:.2f}, Max Drawdown: {maxdd*100:.2f}%\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
